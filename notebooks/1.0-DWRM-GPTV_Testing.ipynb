{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bddfc535-9593-42fe-8b05-4680ba70926a",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "Test out running images to GPT-4 image recognition model for querying them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9005defb-b608-430d-b91a-c384cf8bed2e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eadbe4f-bacf-4f41-9ffa-07e3d1e29d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rich import print\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d580a289-22c0-4108-8318-ef212dbd3914",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1662f8d-4df8-4c2c-b104-5290d53bba7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: python-dotenv\n",
      "Version: 1.0.0\n",
      "Summary: Read key-value pairs from a .env file and set them as environment variables\n",
      "Home-page: https://github.com/theskumar/python-dotenv\n",
      "Author: Saurabh Kumar\n",
      "Author-email: me+github@saurabh-kumar.com\n",
      "License: BSD-3-Clause\n",
      "Location: /Users/davemcrench/anaconda3/lib/python3.11/site-packages\n",
      "Requires: \n",
      "Required-by: anaconda-cloud-auth\n"
     ]
    }
   ],
   "source": [
    "# Make sure we have the version we expect\n",
    "!pip show python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ecd37c8-bf68-4a87-b888-d8add53e3877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417c53fc-2fc9-4f82-9eb6-7d8efdbb54b9",
   "metadata": {},
   "source": [
    "# Check Available Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bf0a4c6-fde0-47a7-bc47-da810a523c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evlens.models.openai_tools import find_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f41e4e39-ee0a-4ac6-93a7-fc30ce8975fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt-4-0613',\n",
       " 'gpt-4',\n",
       " 'gpt-4-1106-preview',\n",
       " 'gpt-4-vision-preview',\n",
       " 'gpt-4-0314']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_models('gpt-4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35510efc-04c8-437d-b138-1799e73a3d1f",
   "metadata": {},
   "source": [
    "`gpt-4-vision-preview` is the one we want!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573d6ad7-56ed-45af-affe-16e3c8a5ac0c",
   "metadata": {},
   "source": [
    "# Query a Sample Image\n",
    "\n",
    "Using the [OpenAI API quickstart](https://platform.openai.com/docs/guides/vision/quick-start) for the topic.\n",
    "\n",
    "Note the [following restrictions](https://platform.openai.com/docs/guides/vision/faq) for GPT-4V as of 12/23/2023:\n",
    "\n",
    "1. They currently support\n",
    "    * PNG (.png)\n",
    "    * JPEG (.jpeg and .jpg)\n",
    "    * WEBP (.webp)\n",
    "    * Non-animated GIF (.gif)\n",
    "2. 20MG limit per image\n",
    "3. Images are deleted on OpenAI's side automatically after being processed\n",
    "4. The model will not process image metadata automatically\n",
    "    * But I imagine that's something you can add as part of your prompt engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6435233-c26f-46d9-b50e-6e60a6ddcc4b",
   "metadata": {},
   "source": [
    "## Our Image\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39c28223-2bf6-4c35-a7c1-b2f7dc4a07cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3485"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evlens.models.openai_tools import count_image_tokens\n",
    "\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
    "count_image_tokens(url=url, high_resolution=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4f6c91-d699-4432-9240-10a6afb12408",
   "metadata": {},
   "source": [
    "## Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133184fc-6fd8-42d5-9eee-54d1bfe74a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evlens.models.openai_tools import ask_simple_vision_question\n",
    "\n",
    "question = \"Whatâ€™s in this image?\"\n",
    "answer = ask_simple_vision_question(question, url)\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e49d8e0-8015-415a-997b-230d2905020a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Important Caveats\n",
    "\n",
    "From OpenAI:\n",
    "\n",
    "> The model is best at answering general questions about what is present in the images. While it does understand the relationship between objects in images, *it is not yet optimized to answer detailed questions about the **location** of certain objects in an image.* For example, you can ask it what color a car is or what some ideas for dinner might be based on what is in you fridge, but if you show it an image of a room and ask it where the chair is, it may not answer the question correctly.\n",
    "\n",
    "Also note the [limitations of the GPT-4V model](https://platform.openai.com/docs/guides/vision/limitations). In particular, this seems relevant for cameras that are observing scenes and may try to capture more info by using wide-angle lenses:\n",
    "\n",
    "> Image shape: The model struggles with panoramic and fisheye images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517d385-8fa2-4eb1-8df9-4bdcd59c8d74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68156d72-fbf7-4ea5-b8b7-9fa99a5a6af6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evlens",
   "language": "python",
   "name": "evlens"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
